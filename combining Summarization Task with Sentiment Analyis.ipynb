{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "560d6ec7-049c-4fed-b4a9-e28b8a3919f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer , AutoModelForSeq2SeqLM , GenerationConfig , pipeline\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from gtts import gTTS\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db84da8-4bde-4a58-8db2-10cbd21baad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "label2id = {\"Negative\":0, \"Positive\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d86093-846a-4619-a54a-1518f68819be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814ab422-ecdd-42b7-8543-d042b1b6e5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Nima\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the base model\n",
    "base_model_checkpoint = \"distilbert-base-uncased\"\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(base_model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7764323-2e05-4710-abfe-c7a1a5c05e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "summarization_model = AutoModelForSeq2SeqLM.from_pretrained(summarization_model_name).to('cuda')\n",
    "summarization_tokenizer = AutoTokenizer.from_pretrained(summarization_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "589fae89-d264-4470-948e-5565623a39b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DistilBertForSequenceClassification(\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x TransformerBlock(\n",
       "              (attention): MultiHeadSelfAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (q_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.01, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (ffn): FFN(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pre_classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the PEFT model with the saved LoRA layers\n",
    "model = PeftModel.from_pretrained(base_model, \"Model\")\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81b2e2ad-ebc0-4732-b6f2-36d71d049be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=250, do_sample=True, temperature=0.74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0f5ad-c6ad-45eb-a134-8446858c41a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    \"That was a good movie.\", \n",
    "    \"i liked that movie.\",\n",
    "    \"The camera quality is disappointing, with grainy pictures, slow autofocus, and washed-out colors.\",\n",
    "    \"The product arrived late with damaged packaging, leading to a disappointing experience.\",\n",
    "    \"it has some software issues and the memory card was crashed\",\n",
    "]\n",
    "\n",
    "positives = []\n",
    "negatives = []\n",
    "\n",
    "# Step 1: Perform Sentiment Analysis and gather negative comments\n",
    "for index, text in enumerate(text_list):\n",
    "    text = text.strip()  # Remove any leading/trailing whitespace\n",
    "    if not text:  # Skip empty lines\n",
    "        continue\n",
    "\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.max(logits, 1).indices\n",
    "    sentiment = \"Negative\" if predictions.tolist()[0] == 0 else \"Positive\"\n",
    "\n",
    "    if sentiment == \"Positive\":\n",
    "        positives.append(text)\n",
    "    else:\n",
    "        negatives.append(f\"Customer {len(negatives) + 1}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5437f4-a8c4-454b-98fd-779104ec4a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all negative comments into a single block of text\n",
    "if negatives:\n",
    "    negative_comments = \"\\n\".join(negatives)\n",
    "else:\n",
    "    negative_comments = \"No negative comments.\"\n",
    "\n",
    "# Human-crafted summary\n",
    "human_summary = \"Customers are frustrated with the poor battery life, subpar camera quality, and issues with shipping and packaging, it has software issues and memory size is small , leading to an overall disappointing experience with the product.\"\n",
    "\n",
    "# Prompt creation for summarization\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation in a more abstract way, focusing on the overall sentiment and key points.\n",
    "\n",
    "{negative_comments}\n",
    "\n",
    "Summary:\n",
    "{human_summary}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218ca8c4-08e7-4b0d-b1cf-e983f947db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input for summarization\n",
    "inputs = summarization_tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate output from the summarization model with adjusted temperature and top-p sampling\n",
    "output = summarization_tokenizer.decode(\n",
    "    summarization_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        generation_config=generation_config     \n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "num_positive = len(positives)\n",
    "num_negative = len(negatives)\n",
    "\n",
    "\n",
    "tts_text = f\"The model-generated summary is: {output}. There are {num_positive} positive and {num_negative} negative comments.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40ca287e-525f-44b3-abb2-4851d92e0450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation in a more abstract way, focusing on the overall sentiment and key points.\n",
      "\n",
      "Customer 1: The camera quality is disappointing, with grainy pictures, slow autofocus, and washed-out colors.\n",
      "Customer 2: The product arrived late with damaged packaging, leading to a disappointing experience.\n",
      "Customer 3: it has some software issues and the memory card was crashed\n",
      "\n",
      "Summary:\n",
      "Customers are frustrated with the poor battery life, subpar camera quality, and issues with shipping and packaging, it has software issues and memory size is small , leading to an overall disappointing experience with the product.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Customers are frustrated with the poor battery life, subpar camera quality, and issues with shipping and packaging, it has software issues and memory size is small , leading to an overall disappointing experience with the product.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      " The camera quality of the product has been disappointing for users. The cameras will have low battery life. There are software issues and small memory cards.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate the speech using gTTS\n",
    "#tts = gTTS(tts_text, lang='en')\n",
    "\n",
    "# Save the speech to a file\n",
    "#tts.save('model_summary_audio.mp3')\n",
    "\n",
    "# Optionally, play the speech directly\n",
    "#os.system(\"mpg321 model_summary_audio.mp3\")\n",
    "\n",
    "dash_line = '-' * 100\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')\n",
    "print(dash_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9377e4f-0450-43e2-bba5-308fab89e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the model and tokenizer are already loaded and moved to GPU\n",
    "model.to('cuda')\n",
    "\n",
    "positives_comments = []\n",
    "negatives_comments = []\n",
    "all_comments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94ce3914-9df2-448b-9f6f-8ab2690311da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model predictions:\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define the list of sentences for sentiment analysis\n",
    "t_list = [\n",
    "    \"That was a good movie.\", \n",
    "    \"i liked that movie.\",\n",
    "    \"The camera quality is disappointing, with grainy pictures, slow autofocus, and washed-out colors.\",\n",
    "    \"The product arrived late with damaged packaging, leading to a disappointing experience.\",\n",
    "    \"it has some software issues and the memory card was crashed\",\n",
    "]\n",
    "\n",
    "print(\"Trained model predictions:\")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "# Prepare the text that will be converted to speech\n",
    "tts_text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "299756fc-f564-4545-bef6-f6532ce64963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That was a good movie. - Positive\n",
      "i liked that movie. - Positive\n",
      "The camera quality is disappointing, with grainy pictures, slow autofocus, and washed-out colors. - Negative\n",
      "The product arrived late with damaged packaging, leading to a disappointing experience. - Negative\n",
      "it has some software issues and the memory card was crashed - Negative\n"
     ]
    }
   ],
   "source": [
    "# Perform sentiment analysis on each sentence\n",
    "for text in t_list:\n",
    "    # Tokenize the sentence and move it to GPU\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Get the model's output logits\n",
    "    logits = model(inputs).logits\n",
    "\n",
    "    # Get the index of the highest logit (the predicted sentiment class)\n",
    "    predictions = torch.max(logits, 1).indices\n",
    "\n",
    "    # Convert the prediction index to the corresponding label\n",
    "    sentiment_label = id2label[predictions.tolist()[0]]\n",
    "    \n",
    "    \n",
    "    # Print the sentence along with its predicted sentiment\n",
    "    result_text = f\"{text} - {sentiment_label}\"\n",
    "    print(result_text)\n",
    "\n",
    "    all_comments.append(result_text)\n",
    "    \n",
    "    if sentiment_label == \"Positive\":\n",
    "        positives_comments.append(sentiment_label)\n",
    "    else:\n",
    "        negatives_comments.append(sentiment_label)\n",
    "\n",
    "    # Append the result to the TTS text\n",
    "    tts_text += result_text + \". \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55d58c9-d838-4e9c-8032-5d1759862aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = (len(positives_comments) / len(all_comments) * 100)\n",
    "rate_text = f\"{int(rate)}% of the users liked this product\"\n",
    "tts_text += rate_text\n",
    "\n",
    "# Convert the text to speech using gTTS\n",
    "#tts = gTTS(tts_text, lang='en')\n",
    "\n",
    "# Save the speech to an MP3 file\n",
    "#tts.save('sentiment_analysis_results.mp3')\n",
    "\n",
    "# Optionally, play the speech directly (this command works on Unix-based systems)\n",
    "#os.system(\"mpg321 sentiment_analysis_results.mp3\")\n",
    "\n",
    "# If you are on Windows, you can play the file with the default media player\n",
    "# os.startfile('sentiment_analysis_results.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c331c016-6072-4352-b732-410ae3cb6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=200, do_sample=True, temperature=0.76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e000adff-4df0-4071-b247-5dcf695ef0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "\n",
      "Person A: Hey, have you finished the report for our project?\n",
      "Person B: Not yet, I'm still working on it. I should have it done by tomorrow.\n",
      "Person A: Great! Do you need any help with it?\n",
      "Person B: I think I'm good, but I'll let you know if I run into any issues. Thanks!.\n",
      "\n",
      "\n",
      "Summary:\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Person B is still working on the project report and expects to finish it by tomorrow. Person A offers help, but Person B doesn't need it at the moment.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      " The conversation on a project report is called \"a project report\" Human constructions may be critical of the project's co-workers. The team members discuss the progress of the report and each other.\n"
     ]
    }
   ],
   "source": [
    "# Self-made dialogue and summary\n",
    "self_made_dialogue = \"\"\"\n",
    "Person A: Hey, have you finished the report for our project?\n",
    "Person B: Not yet, I'm still working on it. I should have it done by tomorrow.\n",
    "Person A: Great! Do you need any help with it?\n",
    "Person B: I think I'm good, but I'll let you know if I run into any issues. Thanks!.\n",
    "\"\"\"\n",
    "\n",
    "self_made_summary = \"Person B is still working on the project report and expects to finish it by tomorrow. Person A offers help, but Person B doesn't need it at the moment.\"\n",
    "\n",
    "# Prompt creation\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{self_made_dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "#{self_made_summary}\n",
    "\n",
    "# Tokenize input\n",
    "inputs = summarization_tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
    "\n",
    "# Generate output from the model\n",
    "output = summarization_tokenizer.decode(\n",
    "    summarization_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        generation_config=generation_config\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# Print results\n",
    "dash_line = '-' * 100\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{self_made_summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b83171c-b36f-479c-88e7-c95741ebb926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
